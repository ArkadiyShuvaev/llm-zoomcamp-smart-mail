{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate a vector retrieval feature against question and answer pair.\n",
    "\n",
    "The goal is to avaluate the concatination of question and answer, the same metrics are used. See the [Evaluate a text retrieval](03_evaluate_text_retrieval.ipynb) file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from services import *\n",
    "from common.settings import Settings\n",
    "from common.client_factory import ClientFactory\n",
    "from services.retrieval_service import RetrievalService\n",
    "from services.reciprocal_rank_fusion_service import ReciprocalRankFusionService\n",
    "from common.sentence_transformer_model_factory import SentenceTransformerModelFactory\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import json\n",
    "import os\n",
    "from typing import Any, Dict, List\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "\n",
    "import importlib\n",
    "import retrieval_evaluation_utils as ev_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'retrieval_evaluation_utils' from '/home/jovyan/work/notebook/utils/retrieval_evaluation_utils.py'>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reload the module\n",
    "importlib.reload(ev_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_with_ids_path = \"/home/jovyan/work/notebook/retrieval_evaluation/dataset_with_doc_ids.csv\"\n",
    "ground_truth_path = \"/home/jovyan/work/notebook/retrieval_evaluation/ground_truth.csv\"\n",
    "evaluation_results_path = \"/home/jovyan/work/notebook/retrieval_evaluation/evaluation_results.csv\"\n",
    "test_name = \"vector_question_answer\"\n",
    "\n",
    "models = [\n",
    "    \"distiluse-base-multilingual-cased-v1\",\n",
    "    \"deepset/gbert-base\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the vector retrieval feature using MRR and HR@k."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the dataset and ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_system</th>\n",
       "      <th>category</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>document_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>evdi</td>\n",
       "      <td>Analysekonzept</td>\n",
       "      <td>Wie läuft der Analyseprozess für Immobilienpro...</td>\n",
       "      <td>Der Analyseprozess bei Engel &amp; Völkers Digital...</td>\n",
       "      <td>f2624f5125f9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>evdi</td>\n",
       "      <td>Analysekonzept</td>\n",
       "      <td>Wie werden die Anlageprojekte bewertet und wie...</td>\n",
       "      <td>Die Bewertung der Anlageprojekte bei Engel &amp; V...</td>\n",
       "      <td>14e6c2e22916</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  source_system         category  \\\n",
       "0          evdi  Analysekonzept   \n",
       "1          evdi  Analysekonzept   \n",
       "\n",
       "                                            question  \\\n",
       "0  Wie läuft der Analyseprozess für Immobilienpro...   \n",
       "1  Wie werden die Anlageprojekte bewertet und wie...   \n",
       "\n",
       "                                              answer   document_id  \n",
       "0  Der Analyseprozess bei Engel & Völkers Digital...  f2624f5125f9  \n",
       "1  Die Bewertung der Anlageprojekte bei Engel & V...  14e6c2e22916  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df: DataFrame\n",
    "\n",
    "if os.path.exists(doc_with_ids_path):\n",
    "    dataset_df = pd.read_csv(doc_with_ids_path, delimiter=\";\")\n",
    "else:\n",
    "    columns = ['source_system', 'category', 'question', 'document_id']\n",
    "    dataset_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "dataset_df[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_system</th>\n",
       "      <th>category</th>\n",
       "      <th>question</th>\n",
       "      <th>document_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>evdi</td>\n",
       "      <td>Analysekonzept</td>\n",
       "      <td>Was sind die wichtigsten Schritte des Analysep...</td>\n",
       "      <td>f2624f5125f9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>evdi</td>\n",
       "      <td>Analysekonzept</td>\n",
       "      <td>Welche externen Partner sind an der Analyse vo...</td>\n",
       "      <td>f2624f5125f9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  source_system         category  \\\n",
       "0          evdi  Analysekonzept   \n",
       "1          evdi  Analysekonzept   \n",
       "\n",
       "                                            question   document_id  \n",
       "0  Was sind die wichtigsten Schritte des Analysep...  f2624f5125f9  \n",
       "1  Welche externen Partner sind an der Analyse vo...  f2624f5125f9  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groud_truth_df: DataFrame\n",
    "if os.path.exists(ground_truth_path):\n",
    "    groud_truth_df = pd.read_csv(ground_truth_path, delimiter=\";\")\n",
    "else:\n",
    "    columns = ['source_system', 'category', 'question', 'document_id']\n",
    "    groud_truth_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "groud_truth_df[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'name': 'a639f0e3a42f', 'cluster_name': 'docker-cluster', 'cluster_uuid': 'aJrw85pRTECvuVEC8ISEHg', 'version': {'number': '8.9.0', 'build_flavor': 'default', 'build_type': 'docker', 'build_hash': '8aa461beb06aa0417a231c345a1b8c38fb498a0d', 'build_date': '2023-07-19T14:43:58.555259655Z', 'build_snapshot': False, 'lucene_version': '9.7.0', 'minimum_wire_compatibility_version': '7.17.0', 'minimum_index_compatibility_version': '7.0.0'}, 'tagline': 'You Know, for Search'})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "settings = Settings()\n",
    "settings.index_name = test_name\n",
    "client_factory = ClientFactory(settings)\n",
    "es_client = client_factory.create_elasticsearch_client()\n",
    "es_client.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define functions to index questions and answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_model() -> SentenceTransformer:\n",
    "    model_factory = SentenceTransformerModelFactory(settings)\n",
    "    embedding_model = model_factory.create_model()\n",
    "    return embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dimensions(embedding_model: SentenceTransformer) -> int:\n",
    "    first_dataset_item = dataset_df.iloc[0]\n",
    "    question = first_dataset_item[\"question\"]\n",
    "    vector_value = embedding_model.encode(question)\n",
    "    dimensions = len(vector_value)\n",
    "    return dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_dataset(dataset_df: DataFrame, embedding_model: SentenceTransformer, dimensions: int) -> None:\n",
    "    index_settings = dict(\n",
    "        settings=dict(\n",
    "            number_of_shards=1,\n",
    "            number_of_replicas=0,\n",
    "        ),\n",
    "        mappings=dict(\n",
    "            properties=dict(\n",
    "                answer=dict(type=\"text\"),\n",
    "                question=dict(type=\"text\"),\n",
    "                category=dict(type=\"text\"),\n",
    "                document_id=dict(type=\"text\"),\n",
    "                answer_instructions=dict(type=\"text\"),\n",
    "                source_system=dict(type=\"keyword\"),\n",
    "                vector_question_answer=dict(\n",
    "                    type=\"dense_vector\",\n",
    "                    dims=dimensions,\n",
    "                    index=True,\n",
    "                    similarity=\"cosine\",\n",
    "                ),\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    if es_client.indices.exists(index=test_name):\n",
    "        es_client.indices.delete(index=test_name)\n",
    "\n",
    "    es_client.indices.create(index=test_name, body=index_settings)\n",
    "\n",
    "    for idx, row in tqdm(dataset_df.iterrows(), total=dataset_df.shape[0]):\n",
    "        document_to_index = row.to_dict()\n",
    "        question_answer = (document_to_index[\"question\"] + \" \" + document_to_index[\"answer\"])\n",
    "        vector = embedding_model.encode(question_answer)\n",
    "        document_to_index[\"vector_question_answer\"] = vector\n",
    "\n",
    "        es_client.index(index=test_name, document=document_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_relevance(groud_truth_df: DataFrame, retrieval_service: RetrievalService) -> List[List[bool]]:\n",
    "    relevance_total: List[List[bool]] = []\n",
    "\n",
    "    for idx, row in tqdm(groud_truth_df.iterrows(), total=groud_truth_df.shape[0]):\n",
    "        doc = row.to_dict()\n",
    "        retrieval_result = retrieval_service.search(doc[\"question\"], 5)\n",
    "        vector_result = retrieval_result.vector_result_items\n",
    "    \n",
    "        relevance: List[bool] = []\n",
    "        for item in vector_result:\n",
    "            relevance.append(item.document_id == doc[\"document_id\"])\n",
    "    \n",
    "        relevance_total.append(relevance)\n",
    "\n",
    "    return relevance_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mrr(dataset: List[List[bool]]):\n",
    "    total_score = 0.0    \n",
    "\n",
    "    for row in dataset:\n",
    "        for idx, value in enumerate(row):\n",
    "            if value is True:\n",
    "                total_score += 1 / (idx + 1)\n",
    "                break\n",
    "\n",
    "    result = total_score / len(dataset)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_hit_rate(dataset: List[List[bool]]) -> float:\n",
    "    total_score = 0.0    \n",
    "\n",
    "    for row in dataset:\n",
    "        if True in row:\n",
    "            total_score += 1\n",
    "            \n",
    "\n",
    "    result = total_score / len(dataset)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_the_result(source_system, calculated_mrr: float, calculated_hit_rate: float, model_name: str, description: str):\n",
    "    df = pd.DataFrame({\n",
    "        \"source_system\": [source_system, source_system],\n",
    "        \"method\": [test_name, test_name],\n",
    "        \"metric\": [\"mrr\", \"HR@K5\"],\n",
    "        \"value\": [calculated_mrr, calculated_hit_rate],\n",
    "        \"model\": [model_name, model_name],\n",
    "        \"description\": [description, description]\n",
    "    })\n",
    "\n",
    "    ev_utils.add_evaluation_results(df, evaluation_results_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]/home/jovyan/.local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "100%|██████████| 87/87 [00:39<00:00,  2.20it/s]\n",
      "100%|██████████| 435/435 [00:49<00:00,  8.75it/s]\n",
      " 50%|█████     | 1/2 [01:59<01:59, 119.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first relevance items '[[True, False, False, False, False], [True, False, False, False, False], [True, False, False, False, False], [True, False, False, False, False], [True, False, False, False, False], [False, True, False, False, False], [True, False, False, False, False], [True, False, False, False, False], [False, True, False, False, False], [True, False, False, False, False]]' for the model distiluse-base-multilingual-cased-v1\n",
      "Hit Rate value: 0.7839080459770115 for the model 'distiluse-base-multilingual-cased-v1'.\n",
      "MRR value: 0.7839080459770115 for the model 'distiluse-base-multilingual-cased-v1'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "100%|██████████| 87/87 [00:14<00:00,  5.90it/s]\n",
      "100%|██████████| 435/435 [00:39<00:00, 11.07it/s]\n",
      "100%|██████████| 2/2 [02:56<00:00, 88.35s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first relevance items '[[True, False, False, False, False], [True, False, False, False, False], [True, False, False, False, False], [True, False, False, False, False], [True, False, False, False, False], [False, True, False, False, False], [True, False, False, False, False], [True, False, False, False, False], [False, True, False, False, False], [True, False, False, False, False]]' for the model deepset/gbert-base\n",
      "Hit Rate value: 0.7839080459770115 for the model 'deepset/gbert-base'.\n",
      "MRR value: 0.7839080459770115 for the model 'deepset/gbert-base'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#### Iterate over all models\n",
    "\n",
    "for model in tqdm(models):\n",
    "    embedding_model = create_embedding_model()\n",
    "    retrieval_service = RetrievalService(es_client, embedding_model, settings)\n",
    "\n",
    "    dimensions = get_dimensions(embedding_model)\n",
    "    index_dataset(dataset_df, embedding_model, dimensions)\n",
    "\n",
    "    relevant_items = calculate_relevance(groud_truth_df, retrieval_service)\n",
    "    print(f\"The first relevance items '{relevant_items[:10]}' for the model {model}\")\n",
    "\n",
    "    calculated_hit_rate = calculate_hit_rate(relevant_items)\n",
    "    print(f\"Hit Rate value: {calculated_hit_rate} for the model '{model}'.\")\n",
    "\n",
    "    calculated_mrr = calculate_mrr(relevant_items)\n",
    "    print(f\"MRR value: {calculated_mrr} for the model '{model}'.\")\n",
    "\n",
    "    save_the_result(dataset_df.iloc[0][\"source_system\"], calculated_mrr, calculated_hit_rate, model, \"vector evaluation against question/answer pair\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean up Elasticsearch index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "if es_client.indices.exists(index=test_name):\n",
    "    es_client.indices.delete(index=test_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
