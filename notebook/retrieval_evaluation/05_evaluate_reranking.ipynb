{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate a reranking feature\n",
    "\n",
    "The approach is to rerank the documents of the text and vector search engines.\n",
    "\n",
    "The embeddings are build using a concatenation of the questions and answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "from services import *\n",
    "from common.settings import Settings\n",
    "from common.client_factory import ClientFactory\n",
    "from services.retrieval_service import RetrievalService\n",
    "from services.reciprocal_rank_fusion_service import ReciprocalRankFusionService\n",
    "from common.sentence_transformer_model_factory import SentenceTransformerModelFactory\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import json\n",
    "import os\n",
    "from typing import Any, Dict, List\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "\n",
    "import importlib\n",
    "import retrieval_evaluation_utils as ev_utils\n",
    "import retrieval_evaluation_ranking_utils as ranking_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'retrieval_evaluation_ranking_utils' from '/home/jovyan/work/notebook/utils/retrieval_evaluation_ranking_utils.py'>"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reload the module\n",
    "importlib.reload(ev_utils)\n",
    "importlib.reload(ranking_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_with_ids_path = \"/home/jovyan/work/notebook/retrieval_evaluation/dataset_with_doc_ids.csv\"\n",
    "ground_truth_path = \"/home/jovyan/work/notebook/retrieval_evaluation/ground_truth.csv\"\n",
    "evaluation_results_path = \"/home/jovyan/work/notebook/retrieval_evaluation/evaluation_results.csv\"\n",
    "test_name = \"reranking_vector_question_answer\"\n",
    "model_name = \"paraphrase-multilingual-mpnet-base-v2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the feature using MRR and HR@k."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the dataset and ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_system</th>\n",
       "      <th>category</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>document_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>evdi</td>\n",
       "      <td>Analysekonzept</td>\n",
       "      <td>Wie läuft der Analyseprozess für Immobilienpro...</td>\n",
       "      <td>Der Analyseprozess bei Engel &amp; Völkers Digital...</td>\n",
       "      <td>f2624f5125f9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>evdi</td>\n",
       "      <td>Analysekonzept</td>\n",
       "      <td>Wie werden die Anlageprojekte bewertet und wie...</td>\n",
       "      <td>Die Bewertung der Anlageprojekte bei Engel &amp; V...</td>\n",
       "      <td>14e6c2e22916</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  source_system         category  \\\n",
       "0          evdi  Analysekonzept   \n",
       "1          evdi  Analysekonzept   \n",
       "\n",
       "                                            question  \\\n",
       "0  Wie läuft der Analyseprozess für Immobilienpro...   \n",
       "1  Wie werden die Anlageprojekte bewertet und wie...   \n",
       "\n",
       "                                              answer   document_id  \n",
       "0  Der Analyseprozess bei Engel & Völkers Digital...  f2624f5125f9  \n",
       "1  Die Bewertung der Anlageprojekte bei Engel & V...  14e6c2e22916  "
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df: DataFrame\n",
    "\n",
    "if os.path.exists(doc_with_ids_path):\n",
    "    dataset_df = pd.read_csv(doc_with_ids_path, delimiter=\";\")\n",
    "else:\n",
    "    columns = [\"source_system\", \"category\", \"question\", \"document_id\"]\n",
    "    dataset_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "dataset_df[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_system</th>\n",
       "      <th>category</th>\n",
       "      <th>question</th>\n",
       "      <th>document_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>evdi</td>\n",
       "      <td>Analysekonzept</td>\n",
       "      <td>Was sind die wichtigsten Schritte des Analysep...</td>\n",
       "      <td>f2624f5125f9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>evdi</td>\n",
       "      <td>Analysekonzept</td>\n",
       "      <td>Welche externen Partner sind an der Analyse vo...</td>\n",
       "      <td>f2624f5125f9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  source_system         category  \\\n",
       "0          evdi  Analysekonzept   \n",
       "1          evdi  Analysekonzept   \n",
       "\n",
       "                                            question   document_id  \n",
       "0  Was sind die wichtigsten Schritte des Analysep...  f2624f5125f9  \n",
       "1  Welche externen Partner sind an der Analyse vo...  f2624f5125f9  "
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groud_truth_df: DataFrame\n",
    "if os.path.exists(ground_truth_path):\n",
    "    groud_truth_df = pd.read_csv(ground_truth_path, delimiter=\";\")\n",
    "else:\n",
    "    columns = [\"source_system\", \"category\", \"question\", \"document_id\"]\n",
    "    groud_truth_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "groud_truth_df[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'name': 'a639f0e3a42f', 'cluster_name': 'docker-cluster', 'cluster_uuid': 'aJrw85pRTECvuVEC8ISEHg', 'version': {'number': '8.9.0', 'build_flavor': 'default', 'build_type': 'docker', 'build_hash': '8aa461beb06aa0417a231c345a1b8c38fb498a0d', 'build_date': '2023-07-19T14:43:58.555259655Z', 'build_snapshot': False, 'lucene_version': '9.7.0', 'minimum_wire_compatibility_version': '7.17.0', 'minimum_index_compatibility_version': '7.0.0'}, 'tagline': 'You Know, for Search'})"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "settings = Settings()\n",
    "settings.index_name = test_name\n",
    "settings.embedding_model_name = model_name\n",
    "client_factory = ClientFactory(settings)\n",
    "es_client = client_factory.create_elasticsearch_client()\n",
    "es_client.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Index questions and answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_factory = SentenceTransformerModelFactory(settings)\n",
    "embedding_model = model_factory.create_model()\n",
    "retrieval_service = RetrievalService(es_client, embedding_model, settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_dataset_item = dataset_df.iloc[0]\n",
    "question = first_dataset_item[\"question\"]\n",
    "vector_value = embedding_model.encode(question)\n",
    "dimensions = len(vector_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87/87 [00:56<00:00,  1.54it/s]\n"
     ]
    }
   ],
   "source": [
    "index_settings = dict(\n",
    "        settings=dict(\n",
    "            number_of_shards=1,\n",
    "            number_of_replicas=0,\n",
    "        ),\n",
    "        mappings=dict(\n",
    "            properties=dict(\n",
    "                answer=dict(type=\"text\"),\n",
    "                question=dict(type=\"text\"),\n",
    "                category=dict(type=\"text\"),\n",
    "                document_id=dict(type=\"text\"),\n",
    "                answer_instructions=dict(type=\"text\"),\n",
    "                source_system=dict(type=\"keyword\"),\n",
    "                vector_question_answer=dict(\n",
    "                    type=\"dense_vector\",\n",
    "                    dims=dimensions,\n",
    "                    index=True,\n",
    "                    similarity=\"cosine\",\n",
    "                ),\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "if es_client.indices.exists(index=test_name):\n",
    "    es_client.indices.delete(index=test_name)\n",
    "\n",
    "es_client.indices.create(index=test_name, body=index_settings)\n",
    "\n",
    "for idx, row in tqdm(dataset_df.iterrows(), total=dataset_df.shape[0]):\n",
    "    document_to_index = row.to_dict()\n",
    "    question_answer = (document_to_index[\"question\"] + \" \" + document_to_index[\"answer\"])\n",
    "    vector = embedding_model.encode(question_answer)\n",
    "    document_to_index[\"vector_question_answer\"] = vector\n",
    "\n",
    "    es_client.index(index=test_name, document=document_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "rrf_service = ReciprocalRankFusionService()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance_total: List[List[bool]] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 435/435 [01:00<00:00,  7.14it/s]\n"
     ]
    }
   ],
   "source": [
    "for idx, row in tqdm(groud_truth_df.iterrows(), total=groud_truth_df.shape[0]):\n",
    "    doc = row.to_dict()\n",
    "    retrieval_result = retrieval_service.search(doc[\"question\"], 5)\n",
    "    \n",
    "    reranking_result = rrf_service.rerank(retrieval_result)\n",
    "    reranked_first5 = reranking_result[:5]\n",
    "\n",
    "    relevance: List[bool] = []\n",
    "    for item in reranked_first5:\n",
    "        relevance.append(item.document_id == doc[\"document_id\"])\n",
    "    \n",
    "    relevance_total.append(relevance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False, True, False, False, False],\n",
      " [False, True, False, False, False],\n",
      " [False, False, True, False, False],\n",
      " [True, False, False, False, False],\n",
      " [True, False, False, False, False],\n",
      " [False, False, False, False, True],\n",
      " [False, False, False, False, True],\n",
      " [False, False, True, False, False],\n",
      " [False, False, True, False, False],\n",
      " [False, True, False, False, False],\n",
      " [False, False, True, False, False],\n",
      " [True, False, False, False, False],\n",
      " [True, False, False, False, False],\n",
      " [False, False, True, False, False],\n",
      " [False, False, False, False, True],\n",
      " [True, False, False, False, False]]\n"
     ]
    }
   ],
   "source": [
    "pprint(relevance_total[:16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mrr(dataset: List[List[bool]]):\n",
    "    total_score = 0.0    \n",
    "\n",
    "    for row in dataset:\n",
    "        for idx, value in enumerate(row):\n",
    "            if value is True:\n",
    "                total_score += 1 / (idx + 1)\n",
    "                break\n",
    "\n",
    "    result = total_score / len(dataset)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_hit_rate(dataset: List[List[bool]]) -> float:\n",
    "    total_score = 0.0    \n",
    "\n",
    "    for row in dataset:\n",
    "        if True in row:\n",
    "            total_score += 1\n",
    "            \n",
    "\n",
    "    result = total_score / len(dataset)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit Rate at k5 value: 0.8\n"
     ]
    }
   ],
   "source": [
    "calculated_hit_rate_at_k5 = ranking_utils.calculate_hit_rate_at_k(relevance_total, 5)\n",
    "print(f\"Hit Rate at k5 value: {calculated_hit_rate_at_k5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit Rate at k3 value: 0.6689655172413793\n"
     ]
    }
   ],
   "source": [
    "calculated_hit_rate_at_k3 = ranking_utils.calculate_hit_rate_at_k(relevance_total, 3)\n",
    "print(f\"Hit Rate at k3 value: {calculated_hit_rate_at_k3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR value: 0.568697318007663\n"
     ]
    }
   ],
   "source": [
    "calculated_mrr = calculate_mrr(relevance_total)\n",
    "print(f\"MRR value: {calculated_mrr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the result fot further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"source_system\": [first_dataset_item[\"source_system\"], first_dataset_item[\"source_system\"], first_dataset_item[\"source_system\"]],\n",
    "    \"method\": [test_name, test_name, test_name],\n",
    "    \"metric\": [\"mrr\", \"HR@K5\", \"HR@K3\"],\n",
    "    \"value\": [calculated_mrr, calculated_hit_rate_at_k5, calculated_hit_rate_at_k3],\n",
    "    \"model\": [settings.embedding_model_name, settings.embedding_model_name, settings.embedding_model_name],\n",
    "    \"description\": [\"reranking against question/answer pair\", \"reranking against question/answer pair\", \"reranking against question/answer pair\"]\n",
    "})\n",
    "\n",
    "ev_utils.add_evaluation_results(df, evaluation_results_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean up Elasticsearch index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "if es_client.indices.exists(index=test_name):\n",
    "    es_client.indices.delete(index=test_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
