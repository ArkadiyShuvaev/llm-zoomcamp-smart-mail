{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate a text retrieval feature\n",
    "\n",
    "The task is to evaluate a text retrieval feature. The two metrics that are used to evaluate the feature are Mean Reciprocal Rank (MRR) and Hit Rate (HR).\n",
    "\n",
    "The MRR is the mean of the reciprocal ranks of the relevant documents (see more information below).\n",
    "\n",
    "The HR is the proportion of queries for which the feature returns at least one relevant document (see more information below).\n",
    "\n",
    "**Mean Reciprocal Rank (MRR)**:\n",
    "   - Evaluates the rank position of the _first_ relevant document.\n",
    "   - Formula: `MRR = (1 / |Q|) * Œ£ (1 / rank_i)` for i = 1 to |Q|\n",
    "\n",
    "**Hit Rate (HR) or Recall at k**:\n",
    "   - Measures the proportion of queries for which at least _one_ relevant document is retrieved in the top k results.\n",
    "   - Formula: `HR@k = (Number of queries with at least one relevant document in top k) / |Q|`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from services import *\n",
    "from common.settings import Settings\n",
    "from common.client_factory import ClientFactory\n",
    "from services.retrieval_service import RetrievalService\n",
    "from services.reciprocal_rank_fusion_service import ReciprocalRankFusionService\n",
    "from common.sentence_transformer_model_factory import SentenceTransformerModelFactory\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import json\n",
    "import os\n",
    "from typing import Any, Dict, List\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "\n",
    "import importlib\n",
    "import retrieval_evaluation_utils as ev_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'retrieval_evaluation_utils' from '/home/jovyan/work/notebook/utils/retrieval_evaluation_utils.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reload the module\n",
    "importlib.reload(ev_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_with_ids_path = \"/home/jovyan/work/notebook/retrieval_evaluation/dataset_with_doc_ids.csv\"\n",
    "ground_truth_path = \"/home/jovyan/work/notebook/retrieval_evaluation/ground_truth.csv\"\n",
    "evaluation_results_path = \"/home/jovyan/work/notebook/retrieval_evaluation/evaluation_results.csv\"\n",
    "test_name = \"text\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the text retrieval feature using MRR and HR@k for the test sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_retrieval_result =  [\n",
    "                                         # MRR        # HR@k=5  # HR@k=1   # HR@k=3\n",
    "    [True, False, False, False, False],  # 1/1 = 1    # 1       # 1        # 1\n",
    "    [False, False, False, False, False], # 0          # 0       # 0        # 0\n",
    "    [False, False, False, False, True],  # 1/5 = 0.2  # 1       # 0        # 0\n",
    "    [False, True, False, False, True]    # 1/2 = 0.5  # 1       # 0        # 1\n",
    "]\n",
    "\n",
    "# MRR   : (1 + 0 + 0.2 + 0.5) / 4 = 1.7 / 4 = 0.425\n",
    "# HR@k=1:  (1 + 0 + 0 + 0) / 4 = 1/4 = 0.25\n",
    "# HR@k=3:  (1 + 0 + 0 + 1) / 4 = 2/4 = 0.5\n",
    "# HR@k=5:  (1 + 0 + 1 + 1) / 4 = 3/4 = 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MRR measures how early the _first_ relevant item appears in a ranked list of results. It calculates the reciprocal of the rank of the first relevant item and averages this across all queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mrr(dataset: List[List[bool]]):\n",
    "    total_score = 0.0    \n",
    "\n",
    "    for row in dataset:\n",
    "        for idx, value in enumerate(row):\n",
    "            if value is True:\n",
    "                total_score += 1 / (idx + 1)\n",
    "                break\n",
    "\n",
    "    result = total_score / len(dataset)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.425\n"
     ]
    }
   ],
   "source": [
    "mrr_value = calculate_mrr(example_retrieval_result)\n",
    "print(mrr_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate Hit Rate at ùëò (HR@k) for the given dataset, we follow the same process: determine if there is at least _one_ relevant (True) item within the top ùëò positions for each query and average these results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_hr_at_k(dataset: List[List[bool]], k: int) -> float:\n",
    "    total_score = 0.0    \n",
    "\n",
    "    for row in dataset:\n",
    "        for i in range(0, k):\n",
    "            if row[i] is True:\n",
    "                total_score += 1\n",
    "                break\n",
    "\n",
    "    result = total_score / len(dataset)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "hr_at_k3 = calculate_hr_at_k(example_retrieval_result, 3)\n",
    "print(hr_at_k3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n"
     ]
    }
   ],
   "source": [
    "hr_at_k5 = calculate_hr_at_k(example_retrieval_result, 5)\n",
    "print(hr_at_k5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the text retrieval feature using MRR and HR@k for the real dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the dataset and ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_system</th>\n",
       "      <th>category</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>document_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>evdi</td>\n",
       "      <td>Analysekonzept</td>\n",
       "      <td>Wie l√§uft der Analyseprozess f√ºr Immobilienpro...</td>\n",
       "      <td>Der Analyseprozess bei Engel &amp; V√∂lkers Digital...</td>\n",
       "      <td>f2624f5125f9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>evdi</td>\n",
       "      <td>Analysekonzept</td>\n",
       "      <td>Wie werden die Anlageprojekte bewertet und wie...</td>\n",
       "      <td>Die Bewertung der Anlageprojekte bei Engel &amp; V...</td>\n",
       "      <td>14e6c2e22916</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  source_system        category  \\\n",
       "0          evdi  Analysekonzept   \n",
       "1          evdi  Analysekonzept   \n",
       "\n",
       "                                            question  \\\n",
       "0  Wie l√§uft der Analyseprozess f√ºr Immobilienpro...   \n",
       "1  Wie werden die Anlageprojekte bewertet und wie...   \n",
       "\n",
       "                                              answer   document_id  \n",
       "0  Der Analyseprozess bei Engel & V√∂lkers Digital...  f2624f5125f9  \n",
       "1  Die Bewertung der Anlageprojekte bei Engel & V...  14e6c2e22916  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df: DataFrame\n",
    "\n",
    "if os.path.exists(doc_with_ids_path):\n",
    "    dataset_df = pd.read_csv(doc_with_ids_path, delimiter=\";\")\n",
    "else:\n",
    "    columns = ['source_system', 'category', 'question', 'document_id']\n",
    "    dataset_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "dataset_df[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_system</th>\n",
       "      <th>category</th>\n",
       "      <th>question</th>\n",
       "      <th>document_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>evdi</td>\n",
       "      <td>Analysekonzept</td>\n",
       "      <td>Was sind die wichtigsten Schritte des Analysep...</td>\n",
       "      <td>f2624f5125f9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>evdi</td>\n",
       "      <td>Analysekonzept</td>\n",
       "      <td>Welche externen Partner sind an der Analyse vo...</td>\n",
       "      <td>f2624f5125f9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  source_system        category  \\\n",
       "0          evdi  Analysekonzept   \n",
       "1          evdi  Analysekonzept   \n",
       "\n",
       "                                            question   document_id  \n",
       "0  Was sind die wichtigsten Schritte des Analysep...  f2624f5125f9  \n",
       "1  Welche externen Partner sind an der Analyse vo...  f2624f5125f9  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groud_truth_df: DataFrame\n",
    "if os.path.exists(ground_truth_path):\n",
    "    groud_truth_df = pd.read_csv(ground_truth_path, delimiter=\";\")\n",
    "else:\n",
    "    columns = ['source_system', 'category', 'question', 'document_id']\n",
    "    groud_truth_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "groud_truth_df[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'name': 'f10eb90c56e6', 'cluster_name': 'docker-cluster', 'cluster_uuid': 'iYx9l4wDS4q30JPqorqswg', 'version': {'number': '8.4.3', 'build_flavor': 'default', 'build_type': 'docker', 'build_hash': '42f05b9372a9a4a470db3b52817899b99a76ee73', 'build_date': '2022-10-04T07:17:24.662462378Z', 'build_snapshot': False, 'lucene_version': '9.3.0', 'minimum_wire_compatibility_version': '7.17.0', 'minimum_index_compatibility_version': '7.0.0'}, 'tagline': 'You Know, for Search'})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "settings = Settings()\n",
    "settings.index_name = test_name\n",
    "client_factory = ClientFactory(settings)\n",
    "es_client = client_factory.create_elasticsearch_client()\n",
    "es_client.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Index questions and answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_factory = SentenceTransformerModelFactory(settings)\n",
    "embedding_model = model_factory.create_model()\n",
    "retrieval_service = RetrievalService(es_client, embedding_model, settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_dataset_item = dataset_df.iloc[0]\n",
    "question = first_dataset_item[\"question\"]\n",
    "# we need dimensions to be compatible with the existing retrieval_service\n",
    "vector_value = embedding_model.encode(question)\n",
    "dimensions = len(vector_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_settings = dict(\n",
    "        settings=dict(\n",
    "            number_of_shards=1,\n",
    "            number_of_replicas=0,\n",
    "        ),\n",
    "        mappings=dict(\n",
    "            properties=dict(\n",
    "                answer=dict(type=\"text\"),\n",
    "                question=dict(type=\"text\"),\n",
    "                category=dict(type=\"text\"),\n",
    "                document_id=dict(type=\"text\"),\n",
    "                answer_instructions=dict(type=\"text\"),\n",
    "                source_system=dict(type=\"keyword\"),\n",
    "                vector_question_answer=dict(\n",
    "                    type=\"dense_vector\",\n",
    "                    dims=dimensions,\n",
    "                    index=True,\n",
    "                    similarity=\"cosine\",\n",
    "                ),\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "if es_client.indices.exists(index=test_name):\n",
    "    es_client.indices.delete(index=test_name)\n",
    "\n",
    "es_client.indices.create(index=test_name, body=index_settings)\n",
    "\n",
    "for idx, row in dataset_df.iterrows():\n",
    "    document_to_index = row.to_dict()\n",
    "    es_client.index(index=test_name, document=document_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance_total: List[List[bool]] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/435 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 435/435 [01:52<00:00,  3.86it/s]\n"
     ]
    }
   ],
   "source": [
    "for idx, row in tqdm(groud_truth_df.iterrows(), total=groud_truth_df.shape[0]):\n",
    "    doc = row.to_dict()\n",
    "    retrieval_result = retrieval_service.search(row[\"question\"], 5)\n",
    "    text_result = retrieval_result.text_result_items\n",
    "    \n",
    "    relevance: List[bool] = []\n",
    "    for item in text_result:\n",
    "        relevance.append(item.document_id == doc[\"document_id\"])\n",
    "    \n",
    "    relevance_total.append(relevance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False, True],\n",
      " [True, False],\n",
      " [False, False],\n",
      " [True, False],\n",
      " [True, False],\n",
      " [False, False],\n",
      " [False, False],\n",
      " [False, True],\n",
      " [False, False],\n",
      " [False, False],\n",
      " [False],\n",
      " [False, False],\n",
      " [],\n",
      " [],\n",
      " [],\n",
      " [True, False]]\n"
     ]
    }
   ],
   "source": [
    "pprint(relevance_total[:16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_hit_rate(dataset: List[List[bool]]) -> float:\n",
    "    total_score = 0.0    \n",
    "\n",
    "    for row in dataset:\n",
    "        if True in row:\n",
    "            total_score += 1\n",
    "            \n",
    "\n",
    "    result = total_score / len(dataset)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit Rate value: 0.6436781609195402\n"
     ]
    }
   ],
   "source": [
    "calculated_hit_rate = calculate_hit_rate(relevance_total)\n",
    "print(f\"Hit Rate value: {calculated_hit_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR value: 0.5620689655172414\n"
     ]
    }
   ],
   "source": [
    "calculated_mrr = calculate_mrr(relevance_total)\n",
    "print(f\"MRR value: {calculated_mrr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"source_system\": [first_dataset_item[\"source_system\"], first_dataset_item[\"source_system\"]],\n",
    "    \"method\": [test_name, test_name],\n",
    "    \"metric\": [\"mrr\", \"HR@K5\"],\n",
    "    \"value\": [calculated_mrr, calculated_hit_rate],\n",
    "    \"model\": [settings.embedding_model_name, settings.embedding_model_name],\n",
    "    \"description\": [\"text evaluation\", \"text evaluation\"]\n",
    "})\n",
    "\n",
    "ev_utils.add_evaluation_results(df, evaluation_results_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean up Elasticsearch index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if es_client.indices.exists(index=test_name):\n",
    "    es_client.indices.delete(index=test_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
